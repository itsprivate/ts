{
  "approved_at_utc": null,
  "subreddit": "askphilosophy",
  "selftext": "The replication crisis is a well-known and ongoing issue in academia. Some meta-studies have put the rate of replicability in certain subdisciplines lower than 50%. But it seems like replicability is a sane epistemological standard, we're trying to figure out how things truly are, not what just happened to occur in a single experiment. But if the chance of some insight being replicable is lower than a coin toss, why accept anything that initially comes out of that domain at all? \n\nIt seems reasonable to reject claims made by experts in that domain by default, and only accept a certain subset that already has been reproduced successfully after some time. But this seems to go strongly against how we treat science, and what we demand of people when we say follow the science. If someone linked an article in /r/science \"experts find that...\" and I replied \"yeah that's a study in a subfield of psychology with a replicability rate of 35%, I don't believe this until I see multiple replications\", I'd probably be banned for science denialism. But would I be unreasonable?",
  "author_fullname": "t2_6ycadjnj",
  "saved": false,
  "mod_reason_title": null,
  "gilded": 0,
  "clicked": false,
  "title": "If we know that less than 50% of results in a discipline are reproducible, why wouldn't it be reasonable to reject its knowledge claims in that domain by default?",
  "link_flair_richtext": [],
  "subreddit_name_prefixed": "r/askphilosophy",
  "hidden": false,
  "pwls": 6,
  "link_flair_css_class": null,
  "downs": 0,
  "top_awarded_type": null,
  "hide_score": false,
  "name": "t3_makgna",
  "quarantine": false,
  "link_flair_text_color": "dark",
  "upvote_ratio": 0.93,
  "author_flair_background_color": null,
  "subreddit_type": "public",
  "ups": 105,
  "total_awards_received": 0,
  "media_embed": {},
  "author_flair_template_id": null,
  "is_original_content": false,
  "user_reports": [],
  "secure_media": null,
  "is_reddit_media_domain": false,
  "is_meta": false,
  "category": null,
  "secure_media_embed": {},
  "link_flair_text": null,
  "can_mod_post": false,
  "score": 105,
  "approved_by": null,
  "author_premium": false,
  "thumbnail": "",
  "edited": false,
  "author_flair_css_class": null,
  "author_flair_richtext": [],
  "gildings": {},
  "content_categories": null,
  "is_self": true,
  "mod_note": null,
  "created": 1616438435,
  "link_flair_type": "text",
  "wls": 6,
  "removed_by_category": null,
  "banned_by": null,
  "author_flair_type": "text",
  "domain": "self.askphilosophy",
  "allow_live_comments": false,
  "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>The replication crisis is a well-known and ongoing issue in academia. Some meta-studies have put the rate of replicability in certain subdisciplines lower than 50%. But it seems like replicability is a sane epistemological standard, we&#39;re trying to figure out how things truly are, not what just happened to occur in a single experiment. But if the chance of some insight being replicable is lower than a coin toss, why accept anything that initially comes out of that domain at all? </p>\n\n<p>It seems reasonable to reject claims made by experts in that domain by default, and only accept a certain subset that already has been reproduced successfully after some time. But this seems to go strongly against how we treat science, and what we demand of people when we say follow the science. If someone linked an article in <a href=\"/r/science\">/r/science</a> &quot;experts find that...&quot; and I replied &quot;yeah that&#39;s a study in a subfield of psychology with a replicability rate of 35%, I don&#39;t believe this until I see multiple replications&quot;, I&#39;d probably be banned for science denialism. But would I be unreasonable?</p>\n</div><!-- SC_ON -->",
  "likes": null,
  "suggested_sort": "confidence",
  "banned_at_utc": null,
  "view_count": null,
  "archived": false,
  "no_follow": false,
  "is_crosspostable": false,
  "pinned": false,
  "over_18": false,
  "awarders": [],
  "media_only": false,
  "can_gild": false,
  "spoiler": false,
  "locked": false,
  "author_flair_text": null,
  "treatment_tags": [],
  "visited": false,
  "removed_by": null,
  "num_reports": null,
  "distinguished": null,
  "subreddit_id": "t5_2sc5r",
  "mod_reason_by": null,
  "removal_reason": null,
  "link_flair_background_color": "",
  "id": "makgna",
  "is_robot_indexable": true,
  "report_reasons": null,
  "author": "hannes_throw_far",
  "discussion_type": null,
  "num_comments": 24,
  "send_replies": true,
  "whitelist_status": "all_ads",
  "contest_mode": false,
  "mod_reports": [],
  "author_patreon_flair": false,
  "author_flair_text_color": null,
  "permalink": "/r/askphilosophy/comments/makgna/if_we_know_that_less_than_50_of_results_in_a/",
  "parent_whitelist_status": "all_ads",
  "stickied": false,
  "url": "https://www.reddit.com/r/askphilosophy/comments/makgna/if_we_know_that_less_than_50_of_results_in_a/",
  "subreddit_subscribers": 173394,
  "created_utc": 1616436389,
  "num_crossposts": 0,
  "media": null,
  "is_video": false,
  "original_created_utc": 1616409635,
  "the_new_excerpt": "The replication crisis is a well-known and ongoing issue in academia. Some\nmeta-studies have put the rate of replicability in certain subdisciplines lower\nthan 50%. But it seems like replicability is a sane epistemological standard,\nwe're trying to figure out how things truly are, not what just…",
  "localize": [
    {
      "locale": "zh",
      "the_new_excerpt": "复制危机是学术界一个众所周知的持续问题。一些\n元研究将某些子学科的可复制率降低了。\n超过50%。但似乎可复制性是一个正常的认识论标准。\n我们试图找出事情的真实情况，而不是... ..."
    },
    {
      "locale": "zh-Hant",
      "the_new_excerpt": "複製危機是學術界一個衆所周知的持續問題。一些\n元研究將某些子學科的可複製率降低了。\n超過50%。但似乎可複製性是一個正常的認識論標準。\n我們試圖找出事情的真實情況，而不是... ..."
    }
  ]
}