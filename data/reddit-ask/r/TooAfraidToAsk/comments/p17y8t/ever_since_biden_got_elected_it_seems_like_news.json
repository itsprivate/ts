{
  "author": "Badparents15",
  "original_created_utc": 1628535510,
  "title": "Ever since Biden got elected, it seems like news about the White House has just disappeared from mainstream media. Is there any truth to this?",
  "created_utc": 1628551718,
  "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>I remember during Obama’s and Trump’s terms they would dominate the news. Every major article would have something about them. In fact, even now if you search up “politics” on Google you’ll most likely find more Trump articles than Biden, OUR PRESIDENT. It just seems odd to me why he isn’t getting much attention like before.</p>\n</div><!-- SC_ON -->",
  "score": 117,
  "permalink": "/r/TooAfraidToAsk/comments/p17y8t/ever_since_biden_got_elected_it_seems_like_news/",
  "subreddit": "TooAfraidToAsk",
  "id": "p17y8t",
  "is_self": true,
  "media": null,
  "is_video": false,
  "the_new_excerpt": "I remember during Obama’s and Trump’s terms they would dominate the news. Every\nmajor article would have something about them. In fact, even now if you search\nup “politics” on Google you’ll most likely find more Trump articles than Biden,\nOUR PRESIDENT. It just seems odd to me why he isn’t getting…"
}