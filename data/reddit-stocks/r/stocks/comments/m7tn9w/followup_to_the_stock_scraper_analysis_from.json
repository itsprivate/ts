{
  "approved_at_utc": null,
  "subreddit": "stocks",
  "selftext": "This is in response [to this post from yesterday](https://www.reddit.com/r/stocks/comments/m71xi8/a_month_of_tracking_stock_scrapers_for/)\n\n&#x200B;\n\nI should have expected it but it didn't occur to me until it started happening, but too many people being in the sheet at the same time locked it up to where most people got errors. I was sending out copies of the sheet but with hundreds of DMs it was getting to be too much to keep up with. I'm interested in presenting this in a way that people can access all the want without it locking up, but although I'm an expert with Excel I don't know Python and don't have any experience auto-publishing stuff online. If anyone can help with that let me know!\n\n&#x200B;\n\nMeanwhile, the most common question I got was how I interpret data so I will give examples from today's data below. Keep in mind that none of this is a clear \"buy me!!\" flashing light, it's just how I interpret the data. I've been getting 10% - 15% returns on about 80% of my picks. The indicators convinced me to buy UWMC, ASO, ASRT, RMBL, CVR, SJR, and AMD within the last week. Of all of them the only one I lost money on was RMBL, the rest all sold at a profit.\n\n&#x200B;\n\nSo what am I looking at today? Before I go into the criteria, here's what the data has shown my so far. This could change after another few weeks of tracking, but as of right now it says:\n\n1. The social media platform that is the most successful in picking stocks is Reddit\n2. Tickers with a Reddit score of 500 or under have been the most likely to go green in the next week\n3. The average % return for those categories is 15%\n4. The average days until it hits max profit is 6 days\n\n&#x200B;\n\n**All that being said, here's the criteria I look for:**\n\n1. Using Unbias data I copy/paste all the Reddit data for the last 24 hours into my excel sheet\n2. From there I filter it down to just items with a score under 500\n3. I have a column that calculates the % increase between yesterday and today so I sort it by % return\n4. I filter out anything that has already had more than 5% growth. Those aren't necessarily bad buys, I just want the full 15% as often as I can get it\n5. I enter each ticker into the Reddit search and manually get rid of anything that isn't a real ticker (sometimes the scraper picks up repeated things that look like tickers but are just phrases)\n6. I note any ticker that is being mentioned across at least 4 different subs. I'm looking for stocks that are getting mentioned all over, not just multiple times in 1 sub\n7. I filter out anything that looks like it's just bots posting all over the place\n\n&#x200B;\n\nFrom there I try narrow it down to 3 - 5 that I feel the best about. Keep in mind I'm trading 100% on sentiment, so I don't do any DD on the company itself. My assumption is that if a lot of people are talking about it then they've done all the DD for me, I just have to follow the crowd and jump on the band wagons that are just starting to move. The strategy has worked pretty well, as of today I'm up about 22% since starting this.\n\n&#x200B;\n\n**How do I put it to use?**\n\n1. I spread the money evenly across 3 - 5 of the tickers that just feel the best to me.\n2. I hold for no longer than a week. I sell somewhere between 10% and 15%, but if it hits 7 days I sell no matter what. Usually I sell after 5 business days, but sometimes I'll hold a week. If I take a loss I take a loss, but usually I'm up\n\n&#x200B;\n\nSo what are my picks today? These are the ones I'm going to look into. I have no idea what will pop and what I'll choose, but these are the ones I'm most curious about based on today's data. I don't know if they're good or not, don't know anything about the companies - only that they fit into the categories that are the most profitable in the spreadsheet\n\n&#x200B;\n\nEDIT: The post was deleted due to mentioning some stocks that it didn't like, [so here's a screenshot of the ones I'll be considering today.](https://imgur.com/gallery/XwhkLWP) Most likely I'm going to throw money evenly across the top 5",
  "author_fullname": "t2_77j5yxdf",
  "saved": false,
  "mod_reason_title": null,
  "gilded": 0,
  "clicked": false,
  "title": "Followup to the Stock Scraper Analysis from yesterday (03/17/21)",
  "link_flair_richtext": [],
  "subreddit_name_prefixed": "r/stocks",
  "hidden": false,
  "pwls": 6,
  "link_flair_css_class": null,
  "downs": 0,
  "top_awarded_type": null,
  "hide_score": false,
  "name": "t3_m7tn9w",
  "quarantine": false,
  "link_flair_text_color": "dark",
  "upvote_ratio": 0.96,
  "author_flair_background_color": null,
  "subreddit_type": "public",
  "ups": 57,
  "total_awards_received": 0,
  "media_embed": {},
  "author_flair_template_id": null,
  "is_original_content": false,
  "user_reports": [],
  "secure_media": null,
  "is_reddit_media_domain": false,
  "is_meta": false,
  "category": null,
  "secure_media_embed": {},
  "link_flair_text": null,
  "can_mod_post": false,
  "score": 57,
  "approved_by": null,
  "author_premium": true,
  "thumbnail": "",
  "edited": false,
  "author_flair_css_class": null,
  "author_flair_richtext": [],
  "gildings": {},
  "content_categories": null,
  "is_self": true,
  "mod_note": null,
  "created": 1616111080,
  "link_flair_type": "text",
  "wls": 6,
  "removed_by_category": null,
  "banned_by": null,
  "author_flair_type": "text",
  "domain": "self.stocks",
  "allow_live_comments": false,
  "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>This is in response <a href=\"https://www.reddit.com/r/stocks/comments/m71xi8/a_month_of_tracking_stock_scrapers_for/\">to this post from yesterday</a></p>\n\n<p>&#x200B;</p>\n\n<p>I should have expected it but it didn&#39;t occur to me until it started happening, but too many people being in the sheet at the same time locked it up to where most people got errors. I was sending out copies of the sheet but with hundreds of DMs it was getting to be too much to keep up with. I&#39;m interested in presenting this in a way that people can access all the want without it locking up, but although I&#39;m an expert with Excel I don&#39;t know Python and don&#39;t have any experience auto-publishing stuff online. If anyone can help with that let me know!</p>\n\n<p>&#x200B;</p>\n\n<p>Meanwhile, the most common question I got was how I interpret data so I will give examples from today&#39;s data below. Keep in mind that none of this is a clear &quot;buy me!!&quot; flashing light, it&#39;s just how I interpret the data. I&#39;ve been getting 10% - 15% returns on about 80% of my picks. The indicators convinced me to buy UWMC, ASO, ASRT, RMBL, CVR, SJR, and AMD within the last week. Of all of them the only one I lost money on was RMBL, the rest all sold at a profit.</p>\n\n<p>&#x200B;</p>\n\n<p>So what am I looking at today? Before I go into the criteria, here&#39;s what the data has shown my so far. This could change after another few weeks of tracking, but as of right now it says:</p>\n\n<ol>\n<li>The social media platform that is the most successful in picking stocks is Reddit</li>\n<li>Tickers with a Reddit score of 500 or under have been the most likely to go green in the next week</li>\n<li>The average % return for those categories is 15%</li>\n<li>The average days until it hits max profit is 6 days</li>\n</ol>\n\n<p>&#x200B;</p>\n\n<p><strong>All that being said, here&#39;s the criteria I look for:</strong></p>\n\n<ol>\n<li>Using Unbias data I copy/paste all the Reddit data for the last 24 hours into my excel sheet</li>\n<li>From there I filter it down to just items with a score under 500</li>\n<li>I have a column that calculates the % increase between yesterday and today so I sort it by % return</li>\n<li>I filter out anything that has already had more than 5% growth. Those aren&#39;t necessarily bad buys, I just want the full 15% as often as I can get it</li>\n<li>I enter each ticker into the Reddit search and manually get rid of anything that isn&#39;t a real ticker (sometimes the scraper picks up repeated things that look like tickers but are just phrases)</li>\n<li>I note any ticker that is being mentioned across at least 4 different subs. I&#39;m looking for stocks that are getting mentioned all over, not just multiple times in 1 sub</li>\n<li>I filter out anything that looks like it&#39;s just bots posting all over the place</li>\n</ol>\n\n<p>&#x200B;</p>\n\n<p>From there I try narrow it down to 3 - 5 that I feel the best about. Keep in mind I&#39;m trading 100% on sentiment, so I don&#39;t do any DD on the company itself. My assumption is that if a lot of people are talking about it then they&#39;ve done all the DD for me, I just have to follow the crowd and jump on the band wagons that are just starting to move. The strategy has worked pretty well, as of today I&#39;m up about 22% since starting this.</p>\n\n<p>&#x200B;</p>\n\n<p><strong>How do I put it to use?</strong></p>\n\n<ol>\n<li>I spread the money evenly across 3 - 5 of the tickers that just feel the best to me.</li>\n<li>I hold for no longer than a week. I sell somewhere between 10% and 15%, but if it hits 7 days I sell no matter what. Usually I sell after 5 business days, but sometimes I&#39;ll hold a week. If I take a loss I take a loss, but usually I&#39;m up</li>\n</ol>\n\n<p>&#x200B;</p>\n\n<p>So what are my picks today? These are the ones I&#39;m going to look into. I have no idea what will pop and what I&#39;ll choose, but these are the ones I&#39;m most curious about based on today&#39;s data. I don&#39;t know if they&#39;re good or not, don&#39;t know anything about the companies - only that they fit into the categories that are the most profitable in the spreadsheet</p>\n\n<p>&#x200B;</p>\n\n<p>EDIT: The post was deleted due to mentioning some stocks that it didn&#39;t like, <a href=\"https://imgur.com/gallery/XwhkLWP\">so here&#39;s a screenshot of the ones I&#39;ll be considering today.</a> Most likely I&#39;m going to throw money evenly across the top 5</p>\n</div><!-- SC_ON -->",
  "likes": null,
  "suggested_sort": null,
  "banned_at_utc": null,
  "view_count": null,
  "archived": false,
  "no_follow": false,
  "is_crosspostable": false,
  "pinned": false,
  "over_18": false,
  "awarders": [],
  "media_only": false,
  "can_gild": false,
  "spoiler": false,
  "locked": false,
  "author_flair_text": null,
  "treatment_tags": [],
  "visited": false,
  "removed_by": null,
  "num_reports": null,
  "distinguished": null,
  "subreddit_id": "t5_2qjfk",
  "mod_reason_by": null,
  "removal_reason": null,
  "link_flair_background_color": "",
  "id": "m7tn9w",
  "is_robot_indexable": true,
  "report_reasons": null,
  "author": "TheIndulgery",
  "discussion_type": null,
  "num_comments": 20,
  "send_replies": true,
  "whitelist_status": "all_ads",
  "contest_mode": false,
  "mod_reports": [],
  "author_patreon_flair": false,
  "author_flair_text_color": null,
  "permalink": "/r/stocks/comments/m7tn9w/followup_to_the_stock_scraper_analysis_from/",
  "parent_whitelist_status": "all_ads",
  "stickied": false,
  "url": "https://www.reddit.com/r/stocks/comments/m7tn9w/followup_to_the_stock_scraper_analysis_from/",
  "subreddit_subscribers": 2480733,
  "created_utc": 1616102591,
  "num_crossposts": 0,
  "media": null,
  "is_video": false,
  "original_created_utc": 1616082280,
  "the_new_excerpt": "This is in response to this post from yesterday\n[https://www.reddit.com/r/stocks/comments/m71xi8/a_month_of_tracking_stock_scrapers_for/]\n\n\n\nI should have expected it but it didn't occur to me until it started happening,\nbut too many people being in the sheet at the same time locked it up to…",
  "localize": [
    {
      "locale": "zh",
      "the_new_excerpt": "这是对昨天这个帖子的回应\n[https://www.reddit.com/r/stocks/comments/m71xi8/a_month_of_tracking_stock_scrapers_for/]\n\n\n\n我应该预料到的，但直到它开始发生，我才想到。\n但太多的人在同一时间的表锁定了... ...",
      "title": "昨天(03/17/21)的股票搜刮器分析的后续工作。"
    },
    {
      "locale": "zh-Hant",
      "the_new_excerpt": "這是對昨天這個帖子的迴應\n[https://www.reddit.com/r/stocks/comments/m71xi8/a_month_of_tracking_stock_scrapers_for/]\n\n\n\n我應該預料到的，但直到它開始發生，我纔想到。\n但太多的人在同一時間的表鎖定了... ...",
      "title": "昨天(03/17/21)的股票搜刮器分析的後續工作。"
    }
  ],
  "source_updated_at": 1616752819547
}